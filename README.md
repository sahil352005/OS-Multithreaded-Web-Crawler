# Web Crawler

This Python application is a simple web crawler that allows you to crawl web pages and extract links. It supports multi-threading for faster crawling and provides options to set the number of threads and the maximum number of links to crawl.

## Features

- Crawl web pages from a specified URL.
- Multi-threaded crawling for improved performance.
- Support for setting the number of threads and maximum number of links to crawl.
- Display of crawling progress and completion status.

## Installation

To run the Web Crawler, you need Python installed on your system. Additionally, you need to install the required dependencies using pip:

pip install beautifulsoup4

## Usage

1. Clone the repository to your local machine:
2. Navigate to the cloned directory:
3. Run the `web_crawler.py` script:
4. Enter the website URL, number of threads, and maximum number of links to crawl in the GUI.
5. Click the "Start Crawling" button to initiate the crawling process.
6. Once the crawling is completed, the GUI will display the total number of pages visited and the number of erroneous links encountered.

## Contributing

Contributions are welcome! If you find any bugs or have suggestions for improvements, please open an issue or submit a pull request.

## License

This project is licensed under the [MIT License](LICENSE).

Hope its helpfull!



